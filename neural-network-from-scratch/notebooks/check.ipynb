{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import forward, random_init, backward\n",
    "from numpy import linalg as LA\n",
    "from keras.datasets import mnist\n",
    "from utils import onehot, crossentropy\n",
    "import copy\n",
    "\n",
    "class GradCheck:\n",
    "    def __init__(self, X, Y, layers_dim, epsilon):\n",
    "        self.layers_dim = layers_dim\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def initialize_params(self):\n",
    "        parameters = random_init(layers_dim)\n",
    "        return parameters\n",
    "    \n",
    "    def dictionary_to_vectors(self, parameters):\n",
    "        cache = []\n",
    "        for _, parameter_values in parameters.items():\n",
    "            flat_mat = parameter_values.flatten()\n",
    "            cache.append(flat_mat)\n",
    "        \n",
    "        return np.concatenate(cache)\n",
    "    \n",
    "    def vectors_to_dictionary(self, vector):\n",
    "        parameters = {}\n",
    "        start = 0\n",
    "        for l in range(1, len(self.layers_dim)):\n",
    "            W_shape = (self.layers_dim[l], self.layers_dim[l-1])\n",
    "            W_size = np.prod(W_shape)\n",
    "            b_shape = (self.layers_dim[l], 1)\n",
    "            b_size = np.prod(b_shape)\n",
    "\n",
    "            parameters[f\"W{l}\"] = vector[start:start+W_size].reshape(W_shape)\n",
    "            start += W_size\n",
    "            parameters[f\"b{l}\"] = vector[start:start+b_size].reshape(b_shape)\n",
    "            start += b_size\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def gradients_zero_like(self, grad_true, parameter_vectorized, epsilon, verbose= True):\n",
    "        # compute numerical gradient for each parameter\n",
    "        gradapprox = np.zeros_like(grad_true)\n",
    "        total_params = parameter_vectorized.shape[0]\n",
    "        original_parameters = copy.deepcopy(parameter_vectorized)\n",
    "        count = 0\n",
    "        count_down = max(1, grad_true.shape[0] // 100)\n",
    "\n",
    "        for i in range(gradapprox.shape[0]):\n",
    "\n",
    "            # theta nudged up by epsilon\n",
    "            theta_plus = copy.deepcopy(original_parameters)\n",
    "\n",
    "            theta_plus[i] += epsilon\n",
    "            parameters_plus = self.vectors_to_dictionary(theta_plus)\n",
    "            y_pred, _ = forward(self.X, parameters_plus)\n",
    "            J_plus = crossentropy(y_pred = y_pred, y_true = self.Y) # obtaining J( theta + epsilon )\n",
    "        \n",
    "            # theta nudged down by epsilon\n",
    "            theta_minus = copy.deepcopy(original_parameters)\n",
    "\n",
    "            theta_minus[i] -= epsilon\n",
    "            parameters_minus = self.vectors_to_dictionary(theta_minus)\n",
    "            y_pred, _ = forward(self.X, parameters_minus)\n",
    "            J_minus = crossentropy(y_pred = y_pred, y_true = self.Y) # obtaining J( theta - epsilon)\n",
    "\n",
    "            # calculating numerical gradients for ith\n",
    "            gradapprox[i] = (J_plus - J_minus) / (2*epsilon)\n",
    "            #print(gradapprox[i])\n",
    "\n",
    "            # verbose\n",
    "            if verbose == True:\n",
    "                if i % 100 == 0:\n",
    "                    count += 1\n",
    "                    count_down -=1\n",
    "                    grad_true_subset = grad_true[:i+1]\n",
    "                    gradapprox_subset = gradapprox[:i+1]\n",
    "                    numerator = LA.norm((gradapprox_subset - grad_true_subset), ord = 2 )\n",
    "                    denominator = LA.norm(gradapprox_subset, ord = 2) + LA.norm(grad_true_subset, ord = 2)\n",
    "                    diff = numerator/denominator\n",
    "                    \n",
    "                    print(f\"Gradient difference at iteration {i}: {diff:.6e}\")\n",
    "                    #print(f\"Grad diff without norm {grad_true_subset - gradapprox_subset}\")\n",
    "                    print(f\"Processed {i} / {total_params} parameters\")\n",
    "                    print(\"#\" * count + \"_\"*count_down)\n",
    "\n",
    "        return gradapprox \n",
    "            \n",
    "    def gradient_checker(self):\n",
    "\n",
    "        # loading in randomly intialized parameters\n",
    "        parameters = self.initialize_params()\n",
    "\n",
    "        # converting dictionary to vectors\n",
    "        parameters_vectorized = self.dictionary_to_vectors(parameters)\n",
    "    \n",
    "        # caching original forward prop parameters for backprop gradients at next step\n",
    "        parameters_original = self.vectors_to_dictionary(parameters_vectorized)\n",
    "        _, cache = forward(self.X, parameters_original)\n",
    "\n",
    "        # caching backprop gradients \n",
    "        grad = backward(y_true = self.Y, cache = cache, params= parameters)\n",
    "        grad_true = self.dictionary_to_vectors(grad)\n",
    "        \n",
    "        gradapprox = self.gradients_zero_like(grad_true= grad_true, parameter_vectorized= parameters_vectorized, epsilon= self.epsilon)\n",
    "        #gradapprox = np.array([gradapprox]) # converted into a numpy array to solve shape problems during broadcasting\n",
    "        #print(gradapprox)\n",
    "        # gradient numerical approximation\n",
    "        numerator = LA.norm((gradapprox - grad_true), ord = 2 )\n",
    "        denominator = LA.norm(gradapprox, ord = 2) + LA.norm(grad_true, ord = 2)\n",
    "        diff = numerator/denominator\n",
    "        #print(grad_true.shape, gradapprox.shape)\n",
    "        return diff\n",
    "    \n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0\n",
    "\n",
    "# Parameters\n",
    "layers_dim = [784, 128, 64, 10]\n",
    "y_test_oh = onehot(y_test, 10)\n",
    "\n",
    "# Use a smaller subset for gradient checking (it's computationally expensive)\n",
    "#X_subset = X_train[:, :3]  # Only first 5 samples\n",
    "#y_subset = y_train_oh[:, :3]\n",
    "\n",
    "gradcheck = GradCheck(X_test, y_test_oh, layers_dim, epsilon=1e-5)\n",
    "diff = gradcheck.gradient_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738eb825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0725fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import forward, random_init, backward\n",
    "from numpy import linalg as LA\n",
    "from keras.datasets import mnist\n",
    "from utils import onehot, crossentropy\n",
    "import copy\n",
    "\n",
    "class GradCheck:\n",
    "    def __init__(self, X, Y, layers_dim, epsilon):\n",
    "        self.layers_dim = layers_dim\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def initialize_params(self):\n",
    "        parameters = random_init(self.layers_dim)  # Use self.layers_dim\n",
    "        return parameters\n",
    "    \n",
    "    def dictionary_to_vectors(self, parameters):\n",
    "        cache = []\n",
    "        for _, parameter_values in parameters.items():\n",
    "            flat_mat = parameter_values.flatten()\n",
    "            cache.append(flat_mat)\n",
    "        \n",
    "        return np.concatenate(cache)\n",
    "    \n",
    "    def vectors_to_dictionary(self, vector):\n",
    "        parameters = {}\n",
    "        start = 0\n",
    "        for l in range(1, len(self.layers_dim)):  # Use self.layers_dim\n",
    "            W_shape = (self.layers_dim[l], self.layers_dim[l-1])\n",
    "            W_size = np.prod(W_shape)\n",
    "            b_shape = (self.layers_dim[l], 1)\n",
    "            b_size = np.prod(b_shape)\n",
    "\n",
    "            parameters[f\"W{l}\"] = vector[start:start+W_size].reshape(W_shape)\n",
    "            start += W_size\n",
    "            parameters[f\"b{l}\"] = vector[start:start+b_size].reshape(b_shape)\n",
    "            start += b_size\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def gradients_zero_like(self, grad_true, parameter_vectorized, epsilon, verbose=True):\n",
    "        # compute numerical gradient for each parameter\n",
    "        gradapprox = np.zeros_like(grad_true)\n",
    "        total_params = parameter_vectorized.shape[0]\n",
    "        \n",
    "        for i in range(gradapprox.shape[0]):\n",
    "            # theta nudged up by epsilon\n",
    "            theta_plus = copy.deepcopy(parameter_vectorized)\n",
    "            theta_plus[i] = parameter_vectorized[i] + epsilon\n",
    "            parameters_plus = self.vectors_to_dictionary(theta_plus)\n",
    "            y_pred, _ = forward(self.X, parameters_plus)\n",
    "            J_plus = crossentropy(y_pred=y_pred, y_true=self.Y)\n",
    "            \n",
    "            # theta nudged down by epsilon\n",
    "            theta_minus = copy.deepcopy(parameter_vectorized)\n",
    "            theta_minus[i] = parameter_vectorized[i] - epsilon\n",
    "            parameters_minus = self.vectors_to_dictionary(theta_minus)\n",
    "            y_pred, _ = forward(self.X, parameters_minus)\n",
    "            J_minus = crossentropy(y_pred=y_pred, y_true=self.Y)\n",
    "\n",
    "            # calculating numerical gradients for ith parameter\n",
    "            gradapprox[i] = (J_plus - J_minus) / (2 * epsilon)\n",
    "\n",
    "            # verbose output - FIXED VERSION\n",
    "            if verbose and i % 20 == 0:\n",
    "                # Use only the computed subset for comparison\n",
    "                grad_true_subset = grad_true[:i+1]\n",
    "                gradapprox_subset = gradapprox[:i+1]\n",
    "                \n",
    "                # Calculate difference using ONLY the subset\n",
    "                numerator = LA.norm(gradapprox_subset - grad_true_subset, ord=2)\n",
    "                denominator = LA.norm(gradapprox_subset, ord=2) + LA.norm(grad_true_subset, ord=2)\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if denominator == 0:\n",
    "                    diff = 0 if numerator == 0 else float('inf')\n",
    "                else:\n",
    "                    diff = numerator / denominator\n",
    "                \n",
    "                print(f'Grad Approx {i} = {gradapprox[i]:.6e}')\n",
    "                print(f'Backprop grad {i} = {grad_true[i]:.6e}')\n",
    "                print(f\"Gradient difference at iteration {i}: {diff:.6e}\")\n",
    "                print(f\"Processed {i+1} / {total_params} parameters\")\n",
    "                progress_bar = \"#\" * (20 * (i+1) // total_params) + \"_\" * (20 - (20 * (i+1) // total_params))\n",
    "                print(f\"[{progress_bar}]\")\n",
    "                print()\n",
    "\n",
    "        return gradapprox \n",
    "            \n",
    "    def gradient_checker(self):\n",
    "        # loading in randomly initialized parameters\n",
    "        parameters = self.initialize_params()\n",
    "\n",
    "        # converting dictionary to vectors\n",
    "        parameters_vectorized = self.dictionary_to_vectors(parameters)\n",
    "    \n",
    "        # caching original forward prop parameters for backprop gradients at next step\n",
    "        parameters_original = self.vectors_to_dictionary(parameters_vectorized)\n",
    "        _, cache = forward(self.X, parameters_original)\n",
    "\n",
    "        # caching backprop gradients \n",
    "        grad = backward(y_true=self.Y, cache=cache, params=parameters)\n",
    "        grad_true = self.dictionary_to_vectors(grad)\n",
    "        \n",
    "        gradapprox = self.gradients_zero_like(\n",
    "            grad_true=grad_true, \n",
    "            parameter_vectorized=parameters_vectorized, \n",
    "            epsilon=self.epsilon\n",
    "        )\n",
    "        \n",
    "        # Final gradient numerical approximation using FULL vectors\n",
    "        numerator = LA.norm(gradapprox - grad_true, ord=2)\n",
    "        denominator = LA.norm(gradapprox, ord=2) + LA.norm(grad_true, ord=2)\n",
    "        \n",
    "        if denominator == 0:\n",
    "            diff = 0 if numerator == 0 else float('inf')\n",
    "        else:\n",
    "            diff = numerator / denominator\n",
    "            \n",
    "        print(f\"\\nFINAL GRADIENT CHECK RESULT: {diff:.6e}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if diff < 1e-7:\n",
    "            print(\"✅ Excellent! Gradients are very close.\")\n",
    "        elif diff < 1e-5:\n",
    "            print(\"✅ Good! Gradients are reasonably close.\")\n",
    "        elif diff < 1e-3:\n",
    "            print(\"⚠️  Warning: Gradients have some discrepancy.\")\n",
    "        else:\n",
    "            print(\"❌ Error: Gradients are significantly different.\")\n",
    "        \n",
    "        return diff\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0\n",
    "\n",
    "# Parameters\n",
    "layers_dim = [784, 4, 3, 10]\n",
    "y_train_oh = onehot(y_train, 10)\n",
    "\n",
    "# Use a smaller subset for gradient checking (it's computationally expensive)\n",
    "X_subset = X_train[:, :5]  # Only first 5 samples\n",
    "y_subset = y_train_oh[:, :5]\n",
    "\n",
    "gradcheck = GradCheck(X_subset, y_subset, layers_dim, epsilon=1e-4)\n",
    "diff = gradcheck.gradient_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672aa97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0\n",
    "\n",
    "# Parameters\n",
    "layers_dim = [784, 4, 3, 10]\n",
    "y_train_oh = onehot(y_train, 10)\n",
    "\n",
    "gradcheck = GradCheck(X_train, y_train_oh, layers_dim, epsilon= 1e-4)\n",
    "\n",
    "parameters = gradcheck.initialize_params()\n",
    "for k, _ in parameters.items():\n",
    "    print(_)\n",
    "parameters_vectorized = gradcheck.dictionary_to_vectors(parameters)\n",
    "print(\"------------------------------------------------\")\n",
    "# Assuming you have initial parameters dictionary params_dict\n",
    "\n",
    "params_reconstructed = gradcheck.vectors_to_dictionary(parameters_vectorized)\n",
    "\n",
    "# Now compare all weights and biases\n",
    "for key in parameters:\n",
    "    print(params_reconstructed[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2c06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
