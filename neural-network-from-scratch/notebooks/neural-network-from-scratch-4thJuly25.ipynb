{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sevBBSgh9g6-",
        "outputId": "05aee15c-9340-431d-b25d-98198b8336cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0],-1).T\n",
        "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
        "\n",
        "# normalize\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# activation functions\n",
        "def sigmoid(z):\n",
        "  f = 1/(1+np.exp(-z))\n",
        "  return f\n",
        "\n",
        "def softmax(z):\n",
        "  f = np.exp(z - np.max(z, axis = 0 , keepdims = True))\n",
        "  return f / np.sum(f,axis = 0, keepdims = True)\n",
        "\n",
        "def relu(z):\n",
        "   return np.maximum(0,z)\n",
        "\n",
        "def random_init(layers_dim):\n",
        "  np.random.seed(0)\n",
        "  params = {}\n",
        "\n",
        "  L = len(layers_dim) - 1\n",
        "\n",
        "  for i in range(1, L+1):\n",
        "     params[\"W\" + str(i)]= np.random.randn(layers_dim[i], layers_dim[i-1]) * np.sqrt(2. / layers_dim[i-1])\n",
        "     params[\"b\" + str(i)]= np.zeros((layers_dim[i],1))\n",
        "  return params\n",
        "\n",
        "def forward(X, params):\n",
        "\n",
        "  z1 = params[\"W1\"] @ X + params[\"b1\"]\n",
        "  a1 = relu(z1)\n",
        "\n",
        "  z2 = params[\"W2\"] @ a1 + params[\"b2\"]\n",
        "  a2 = relu(z2)\n",
        "\n",
        "  z3 = params[\"W3\"] @ a2 + params[\"b3\"]\n",
        "  a3 = softmax(z3)\n",
        "\n",
        "\n",
        "  cache = {\"X\": X,\n",
        "           \"A1\":a1, \"A2\":a2, \"A3\":a3,\n",
        "           \"Z1\":z1, \"Z2\": z2, \"Z3\": z3\n",
        "           }\n",
        "  y_pred = a3\n",
        "  return y_pred, cache\n",
        "\n",
        "# Cross Entropy Loss\n",
        "def crossentropy(y_pred, y_true):\n",
        "   sample =-np.sum(y_true * np.log(y_pred + 1e-8) , axis = 0)\n",
        "   loss = np.mean(sample)\n",
        "\n",
        "   return loss\n",
        "\n",
        "def relu_derivative(Z):\n",
        "  return Z > 0\n",
        "\n",
        "def backward(y_true, cache, params):\n",
        "  m = y_true.shape[1]\n",
        "  dz3 = cache[\"A3\"] - y_true\n",
        "  dw3 = (1/m) * dz3 @ cache[\"A2\"].T\n",
        "  db3 = np.sum((1/m) * dz3, axis = 1, keepdims = True)\n",
        "\n",
        "  da2 = params[\"W3\"].T @ dz3\n",
        "  dz2 = da2 * relu_derivative(cache[\"Z2\"])\n",
        "  dw2 = (1/m) * dz2 @ cache[\"A1\"].T\n",
        "  db2 = np.sum((1/m) * dz2, axis = 1, keepdims = True)\n",
        "\n",
        "  da1 = params[\"W2\"].T @ dz2\n",
        "  dz1 = da1 * relu_derivative(cache[\"Z1\"])\n",
        "  dw1 = (1/m) * dz1 @ cache[\"X\"].T\n",
        "  db1 = np.sum((1/m) * dz1, axis = 1, keepdims = True)\n",
        "\n",
        "  gradients = { \"dW3\":dw3, \"db3\":db3,\n",
        "  \"dW2\":dw2, \"db2\":db2,\n",
        "  \"dW1\":dw1, \"db1\":db1 }\n",
        "\n",
        "  return gradients\n",
        "\n",
        "def onehot(y, classes):\n",
        "  one_hot_y = np.zeros((classes, y.size))\n",
        "  one_hot_y[y, np.arange(y.size)] = 1\n",
        "  return one_hot_y\n",
        "\n",
        "# paramaters\n",
        "layers_dim = [784,128,64,10]\n",
        "y_train_oh = onehot(y_train, 10)\n",
        "\n",
        "# training\n",
        "def train(lr, X, y_true, layers_dim, epochs):\n",
        "\n",
        "  # Random Intialization of w, b\n",
        "  params = random_init(layers_dim)\n",
        "\n",
        "  # Mini batch\n",
        "  batch_size = 64\n",
        "  m = X.shape[1]\n",
        "  num_indices = m // batch_size\n",
        "  for epoch in range(epochs):\n",
        "    # shuffle on each epoch\n",
        "    perm = np.random.permutation(m)\n",
        "    X_shuffled = X[:,perm]\n",
        "    y_shuffled = y_true[:,perm]\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(num_indices):\n",
        "     start = i * batch_size\n",
        "     end = start + batch_size\n",
        "     X_batch = X_shuffled[:,start:end]\n",
        "     y_batch = y_shuffled[:,start:end]\n",
        "     # forward pass\n",
        "     y_pred, cache = forward(X_batch, params)\n",
        "\n",
        "     # computing loss\n",
        "     loss = crossentropy(y_pred, y_batch)\n",
        "     epoch_loss += loss\n",
        "     #print(f\"Epochs : {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "     # backward prop\n",
        "     gradients = backward(y_batch, cache, params)\n",
        "\n",
        "     # update weights\n",
        "     params[\"W1\"] -= lr*gradients[\"dW1\"]\n",
        "     params[\"b1\"] -= lr*gradients[\"db1\"]\n",
        "\n",
        "     params[\"W2\"] -= lr*gradients[\"dW2\"]\n",
        "     params[\"b2\"] -= lr*gradients[\"db2\"]\n",
        "\n",
        "     params[\"W3\"] -= lr*gradients[\"dW3\"]\n",
        "     params[\"b3\"] -= lr*gradients[\"db3\"]\n",
        "    print(f\"Epochs : {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FOqinwQW9zav",
        "outputId": "ade1d1fd-487b-458c-eed6-4615a78cd72e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs : 1, Loss: 300.6419\n",
            "Epochs : 2, Loss: 139.7600\n",
            "Epochs : 3, Loss: 100.4822\n",
            "Epochs : 4, Loss: 78.2120\n",
            "Epochs : 5, Loss: 64.3613\n",
            "Epochs : 6, Loss: 53.5817\n",
            "Epochs : 7, Loss: 45.2224\n",
            "Epochs : 8, Loss: 38.4811\n",
            "Epochs : 9, Loss: 32.2295\n",
            "Epochs : 10, Loss: 27.7540\n",
            "Epochs : 11, Loss: 22.7725\n",
            "Epochs : 12, Loss: 20.3815\n",
            "Epochs : 13, Loss: 16.8596\n",
            "Epochs : 14, Loss: 14.3125\n",
            "Epochs : 15, Loss: 11.3613\n",
            "Epochs : 16, Loss: 9.4168\n",
            "Epochs : 17, Loss: 8.4446\n",
            "Epochs : 18, Loss: 6.4401\n",
            "Epochs : 19, Loss: 4.9158\n",
            "Epochs : 20, Loss: 4.2946\n",
            "Epochs : 21, Loss: 3.5673\n",
            "Epochs : 22, Loss: 2.9634\n",
            "Epochs : 23, Loss: 2.6185\n",
            "Epochs : 24, Loss: 2.3308\n",
            "Epochs : 25, Loss: 2.0620\n",
            "Epochs : 26, Loss: 1.8381\n",
            "Epochs : 27, Loss: 1.7116\n",
            "Epochs : 28, Loss: 1.4964\n",
            "Epochs : 29, Loss: 1.3686\n",
            "Epochs : 30, Loss: 1.2535\n",
            "Epochs : 31, Loss: 1.1411\n",
            "Epochs : 32, Loss: 1.0821\n",
            "Epochs : 33, Loss: 1.0131\n",
            "Epochs : 34, Loss: 0.9404\n",
            "Epochs : 35, Loss: 0.8881\n",
            "Epochs : 36, Loss: 0.8413\n",
            "Epochs : 37, Loss: 0.7945\n",
            "Epochs : 38, Loss: 0.7577\n",
            "Epochs : 39, Loss: 0.7221\n",
            "Epochs : 40, Loss: 0.6882\n",
            "Epochs : 41, Loss: 0.6601\n",
            "Epochs : 42, Loss: 0.6315\n",
            "Epochs : 43, Loss: 0.6020\n",
            "Epochs : 44, Loss: 0.5813\n",
            "Epochs : 45, Loss: 0.5581\n",
            "Epochs : 46, Loss: 0.5402\n",
            "Epochs : 47, Loss: 0.5189\n",
            "Epochs : 48, Loss: 0.5029\n",
            "Epochs : 49, Loss: 0.4863\n",
            "Epochs : 50, Loss: 0.4682\n",
            "Epochs : 51, Loss: 0.4540\n",
            "Epochs : 52, Loss: 0.4413\n",
            "Epochs : 53, Loss: 0.4274\n",
            "Epochs : 54, Loss: 0.4142\n",
            "Epochs : 55, Loss: 0.4024\n",
            "Epochs : 56, Loss: 0.3909\n",
            "Epochs : 57, Loss: 0.3813\n",
            "Epochs : 58, Loss: 0.3709\n",
            "Epochs : 59, Loss: 0.3603\n",
            "Epochs : 60, Loss: 0.3515\n",
            "Epochs : 61, Loss: 0.3424\n",
            "Epochs : 62, Loss: 0.3341\n",
            "Epochs : 63, Loss: 0.3250\n",
            "Epochs : 64, Loss: 0.3187\n",
            "Epochs : 65, Loss: 0.3115\n",
            "Epochs : 66, Loss: 0.3038\n",
            "Epochs : 67, Loss: 0.2991\n",
            "Epochs : 68, Loss: 0.2915\n",
            "Epochs : 69, Loss: 0.2850\n",
            "Epochs : 70, Loss: 0.2792\n",
            "Epochs : 71, Loss: 0.2721\n",
            "Epochs : 72, Loss: 0.2678\n",
            "Epochs : 73, Loss: 0.2624\n",
            "Epochs : 74, Loss: 0.2572\n",
            "Epochs : 75, Loss: 0.2525\n",
            "Epochs : 76, Loss: 0.2467\n",
            "Epochs : 77, Loss: 0.2428\n",
            "Epochs : 78, Loss: 0.2376\n",
            "Epochs : 79, Loss: 0.2341\n",
            "Epochs : 80, Loss: 0.2292\n",
            "Epochs : 81, Loss: 0.2252\n",
            "Epochs : 82, Loss: 0.2214\n",
            "Epochs : 83, Loss: 0.2174\n",
            "Epochs : 84, Loss: 0.2142\n",
            "Epochs : 85, Loss: 0.2102\n",
            "Epochs : 86, Loss: 0.2074\n",
            "Epochs : 87, Loss: 0.2033\n",
            "Epochs : 88, Loss: 0.2007\n",
            "Epochs : 89, Loss: 0.1973\n",
            "Epochs : 90, Loss: 0.1940\n",
            "Epochs : 91, Loss: 0.1908\n",
            "Epochs : 92, Loss: 0.1882\n",
            "Epochs : 93, Loss: 0.1857\n",
            "Epochs : 94, Loss: 0.1824\n",
            "Epochs : 95, Loss: 0.1799\n",
            "Epochs : 96, Loss: 0.1772\n",
            "Epochs : 97, Loss: 0.1744\n",
            "Epochs : 98, Loss: 0.1723\n",
            "Epochs : 99, Loss: 0.1696\n",
            "Epochs : 100, Loss: 0.1673\n",
            "Test Accuracy: 98.13 %\n",
            "Train Accuracy: 100.0 %\n"
          ]
        }
      ],
      "source": [
        "trained_params = train(0.1, X_train, y_train_oh,layers_dim, 100)\n",
        "\n",
        "def accuracy(X, y_true, params):\n",
        "  y_pred, _ = forward(X, params)\n",
        "  preds = np.argmax(y_pred, axis=0)\n",
        "  return np.mean(preds == y_true)\n",
        "\n",
        "acc = accuracy(X_test, y_test, trained_params)\n",
        "print(\"Test Accuracy:\", acc*100,\"%\")\n",
        "acc = accuracy(X_train, y_train, trained_params)\n",
        "print(\"Train Accuracy:\", acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRSEn2IRjdt3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}